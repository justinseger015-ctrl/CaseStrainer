"""
Canonical Case Name and Date Retrieval Implementation
Provides authoritative case information from multiple legal databases
"""

import re
from src.config import DEFAULT_REQUEST_TIMEOUT, COURTLISTENER_TIMEOUT, CASEMINE_TIMEOUT, WEBSEARCH_TIMEOUT, SCRAPINGBEE_TIMEOUT

import logging
import requests
import time
from typing import Dict, Optional, Any, List
from urllib.parse import quote_plus
import json
from functools import lru_cache
from dataclasses import dataclass
try:
    from .config import get_config_value
except ImportError:
    from config import get_config_value
import os
from src.comprehensive_websearch_engine import search_cluster_for_canonical_sources
import concurrent.futures
import sys

logger = logging.getLogger(__name__)

fallback_logger = logging.getLogger('fallback_usage')
fallback_logger.setLevel(logging.INFO)

try:
    logs_dir = 'logs'
    if not os.path.exists(logs_dir):
        os.makedirs(logs_dir, exist_ok=True)
    fallback_log_path = os.path.join(logs_dir, 'fallback_usage.log')
    fallback_handler = logging.FileHandler(fallback_log_path)
    fallback_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    ))
    fallback_logger.addHandler(fallback_handler)
except Exception as e:
    logger.warning(f"Could not create fallback log file: {e}")
    logger.error(f"FALLBACK LOGGING ERROR: Could not create fallback log file: {e}")

def log_fallback_usage(citation: str, fallback_type: str, reason: str, context: Optional[Dict] = None):
    """Log fallback usage to dedicated log file"""
    try:
        log_entry = {
            'timestamp': time.time(),
            'citation': citation,
            'fallback_type': fallback_type,
            'reason': reason,
            'context': context or {}
        }
        fallback_logger.info(f"FALLBACK_USED: {json.dumps(log_entry)}")
    except Exception as e:
        logger.error(f"Failed to log fallback usage: {e}")

@dataclass
class CanonicalResult:
    """Structured result for canonical case information"""
    case_name: Optional[str] = None
    date: Optional[str] = None
    court: Optional[str] = None
    docket_number: Optional[str] = None
    url: Optional[str] = None
    source: str = "unknown"
    confidence: float = 0.0
    verified: bool = False
    parallel_citations: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.parallel_citations is None:
            self.parallel_citations = []

class CanonicalCaseNameService:
    """
    Service for retrieving canonical case names and dates from legal databases
    """
    
    def __init__(self):
        self.courtlistener_api_key = self._get_config_value("COURTLISTENER_API_KEY")
        self.caselaw_api_key = self._get_config_value("CASELAW_API_KEY")
        self.westlaw_api_key = self._get_config_value("WESTLAW_API_KEY")
        
        self.last_request_time = {}
        self.min_request_interval = 1.0  # seconds between requests
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'CaseStrainer/2.0 Legal Citation Processor'
        })
        
        self.citation_patterns = {
            'wn2d': r'(\d+)\s+Wn\.2d\s+(\d+)',
            'wn_app': r'(\d+)\s+Wn\.\s*App\.\s+(\d+)', 
            'p3d': r'(\d+)\s+P\.3d\s+(\d+)',
            'p2d': r'(\d+)\s+P\.2d\s+(\d+)',
            'us': r'(\d+)\s+U\.S\.\s+(\d+)',
            'f3d': r'(\d+)\s+F\.3d\s+(\d+)',
            'f2d': r'(\d+)\s+F\.2d\s+(\d+)',
        }
        
        self._cached_lookup.cache_clear()
    
    def _get_config_value(self, key: str) -> Optional[str]:
        """Get configuration value from environment or config file"""
        import os
        try:
            value = os.environ.get(key)
            if value:
                return value
            
            return get_config_value(key)
        except ImportError:
            return os.environ.get(key)
    
    def _normalize_citation_comprehensive(self, citation: str, purpose: str = "verification") -> str:
        """
        Normalize citation format for better API matching.
        
        Args:
            citation: The citation to normalize
            purpose: The purpose of normalization ("general", "bluebook", "verification", "comparison")
            
        Returns:
            The normalized citation
        """
        if not citation:
            return ""
        
        normalized = re.sub(r'\s+', ' ', citation.strip())
        
        normalized = re.sub(r'Wash\.\s*2d', 'Wn.2d', normalized)
        normalized = re.sub(r'Wash\.\s*App\.', 'Wn. App.', normalized)
        
        normalized = re.sub(r'P\.\s*3d', 'P.3d', normalized)
        normalized = re.sub(r'P\.\s*2d', 'P.2d', normalized)
        
        return normalized
    
    def _extract_citation_components(self, citation: str) -> Dict[str, str]:
        """Extract volume, reporter, and page from citation"""
        components = {'volume': '', 'reporter': '', 'page': ''}
        
        for pattern_name, pattern in self.citation_patterns.items():
            match = re.search(pattern, citation)
            if match:
                components['volume'] = match.group(1)
                components['page'] = match.group(2)
                
                reporter_map = {
                    'wn2d': 'Wn.2d',
                    'wn_app': 'Wn. App.',
                    'p3d': 'P.3d',
                    'p2d': 'P.2d',
                    'us': 'U.S.',
                    'f3d': 'F.3d',
                    'f2d': 'F.2d'
                }
                components['reporter'] = reporter_map.get(pattern_name, '')
                break
        
        return components
    
    def _rate_limit(self, service: str):
        """Implement rate limiting for API requests"""
        now = time.time()
        last_time = self.last_request_time.get(service, 0)
        
        time_since_last = now - last_time
        if time_since_last < self.min_request_interval:
            sleep_time = self.min_request_interval - time_since_last
            time.sleep(sleep_time)
        
        self.last_request_time[service] = time.time()
    
    @lru_cache(maxsize=1000)
    def _cached_lookup(self, normalized_citation: str) -> Optional[CanonicalResult]:
        """Cached lookup to avoid repeated API calls"""
        return self._perform_lookup(normalized_citation)
    
    def _perform_lookup(self, citation: str) -> Optional[CanonicalResult]:
        """Perform the actual lookup across multiple services"""
        
        services = [
            ('courtlistener', self._lookup_courtlistener),
            ('web_sources', self._lookup_web_sources)   # Web search (can target CAP, Westlaw, etc.)
        ]
        
        failed_services = []
        for service_name, lookup_func in services:
            try:
                self._rate_limit(service_name)
                result = lookup_func(citation)
                from src.canonical_case_name_service import is_valid_case_name
                if result and result.case_name:
                    if not is_valid_case_name(result.case_name):
                        logger.warning(f"[HARDFILTER] Rejected result from {service_name} for '{citation or ''}': '{result.case_name}' is not a valid case name (hard filter)")
                        continue
                    else:
                        logger.info(f"[HARDFILTER] Accepted result from {service_name} for '{citation or ''}': '{result.case_name}' is a valid case name (hard filter)")
                if result and result.case_name:
                    logger.info(f"Found canonical result via {service_name}: {result.case_name}")
                    if service_name == 'web_sources':
                        log_fallback_usage(
                            citation=citation,
                            fallback_type='canonical_lookup',
                            reason=f"Primary service (courtlistener) failed, using {service_name}",
                            context={'result_case_name': result.case_name, 'result_source': result.source}
                        )
                        result.source = f"fallback: {result.source}"
                    return result
                else:
                    failed_services.append(f"{service_name}(no_result)")
            except Exception as e:
                failed_services.append(f"{service_name}(error:{str(e)})")
                logger.warning(f"Lookup failed for {service_name}: {e}")
                continue
        
        log_fallback_usage(
            citation=citation,
            fallback_type='canonical_lookup',
            reason=f"All services failed: {', '.join(failed_services)}",
            context={'failed_services': failed_services}
        )
        return None
    
    def _lookup_courtlistener(self, citation: str) -> Optional[CanonicalResult]:
        """Lookup using CourtListener API"""
        if not self.courtlistener_api_key:
            return None
        
        try:
            components = self._extract_citation_components(citation)
            if not all(components.values()):
                return None
            
            citation_text = f'{components["volume"]} {components["reporter"]} {components["page"]}'
            query = f'citation:"{citation_text}"'  # Search specifically in citation field
            
            url = "https://www.courtlistener.com/api/rest/v4/search/"
            headers = {"Authorization": f"Token {self.courtlistener_api_key}"}
            params = {
                "q": query,
                "type": "o",  # Opinions
                "format": "json",
                "page_size": 10,
                "order_by": "score desc"
            }
            
            response = self.session.get(url, headers=headers, params=params, timeout=WEBSEARCH_TIMEOUT)
            response.raise_for_status()
            
            data = response.json()
            results = data.get('results', [])
            
            for result in results:
                citation_string = result.get('citation', [])
                if isinstance(citation_string, list):
                    citation_string = ' '.join(citation_string)
                
                if citation.lower() in str(citation_string).lower():
                    return CanonicalResult(
                        case_name=result.get('caseName', ''),
                        date=result.get('dateFiled', ''),
                        court=result.get('court', ''),
                        docket_number=result.get('docketNumber', ''),
                        url=result.get('absolute_url', ''),
                        source='CourtListener',
                        confidence=0.9,
                        verified=True,
                        parallel_citations=result.get('citation', [])
                    )
                
                if (components['volume'] in str(citation_string) and 
                    components['page'] in str(citation_string) and
                    components['reporter'] in str(citation_string)):
                    
                    return CanonicalResult(
                        case_name=result.get('caseName', ''),
                        date=result.get('dateFiled', ''),
                        court=result.get('court', ''),
                        docket_number=result.get('docketNumber', ''),
                        url=result.get('absolute_url', ''),
                        source='CourtListener',
                        confidence=0.9,
                        verified=True,
                        parallel_citations=result.get('citation', [])
                    )
            
            return None
            
        except Exception as e:
            logger.error(f"CourtListener lookup failed: {e}")
            return None
    
    def _lookup_caselaw_access(self, citation: str) -> Optional[CanonicalResult]:
        """Lookup using Caselaw Access Project API"""
        try:
            url = "https://api.case.law/v1/cases/"
            params = {
                "cite": citation,
                "format": "json",
                "full_case": "false"
            }
            
            if self.caselaw_api_key:
                headers = {"Authorization": f"Token {self.caselaw_api_key}"}
            else:
                headers = {}
            
            response = self.session.get(url, headers=headers, params=params, timeout=COURTLISTENER_TIMEOUT)
            response.raise_for_status()
            
            data = response.json()
            results = data.get('results', [])
            
            if results:
                result = results[0]  # Take first result
                
                date = result.get('decision_date', '')
                if date:
                    year_match = re.search(r'(\d{4})', date)
                    if year_match:
                        date = year_match.group(1)
                
                return CanonicalResult(
                    case_name=result.get('name_abbreviation', ''),
                    date=date,
                    court=result.get('court', {}).get('name', ''),
                    docket_number=result.get('docket_number', ''),
                    url=result.get('frontend_url', ''),
                    source='Caselaw Access Project',
                    confidence=0.85,
                    verified=True,
                    parallel_citations=result.get('citations', [])
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Caselaw Access lookup failed: {e}")
            return None
    
    def _lookup_westlaw(self, citation: str) -> Optional[CanonicalResult]:
        """Lookup using Westlaw API or fallback to canonical verification."""
        try:
            try:
                from src.unified_citation_processor_v2 import UnifiedCitationProcessorV2
                PROCESSOR_AVAILABLE = True
            except ImportError:
                PROCESSOR_AVAILABLE = False
                logger.warning("UnifiedCitationProcessorV2 not available")
                
            if PROCESSOR_AVAILABLE:
                processor = UnifiedCitationProcessorV2()
                try:
                    from src.models import CitationResult
                    citation_result = CitationResult(citation=citation)
                    verified = processor._verify_citation_with_courtlistener(citation_result)
                    if verified:
                        return CanonicalResult(
                            case_name=citation_result.canonical_name,
                            date=citation_result.canonical_date,
                            url=citation_result.url,
                            source="Westlaw (via UnifiedProcessor)",
                            confidence=0.8
                        )
                except Exception as e:
                    logger.warning(f"Failed to verify citation {citation or ''}: {e}")
            
            if not self.westlaw_api_key:
                return None
            
            url = "https://api.westlaw.com/v1/cases"
            headers = {
                "Authorization": f"Bearer {self.westlaw_api_key}",
                "Content-Type": "application/json"
            }
            params = {"q": citation}
            
            response = self.session.get(url, headers=headers, params=params, timeout=COURTLISTENER_TIMEOUT)
            response.raise_for_status()
            
            data = response.json()
            
            return None
            
        except Exception as e:
            logger.error(f"Westlaw lookup failed: {e}")
            return None
    
    def _lookup_web_sources(self, citation: str) -> Optional[CanonicalResult]:
        """Lookup using intelligent web scraping with EnhancedLegalSearchEngine and multiple engines, with cluster batching and a global timeout. Site-specific legal DBs are tried before general search engines."""
        try:
            import logging
            import asyncio
            import time
            from src.enhanced_legal_search_engine import EnhancedLegalSearchEngine
            from src.comprehensive_websearch_engine import ComprehensiveWebSearchEngine
            logger = logging.getLogger(__name__)

            import subprocess
            import sys
            try:
                result = subprocess.run([
                    sys.executable, "playwright_search_justia.py", citation
                ], capture_output=True, text=True, timeout=12)
                output = result.stdout.strip()
                if "Case name:" in output:
                    case_name = output.split("Case name:")[-1].strip()
                    if case_name and case_name.lower() != 'none':
                        return CanonicalResult(
                            case_name=case_name,
                            date=None,
                            url=None,
                            source="justia (playwright)",
                            confidence=0.85,
                            verified=True
                        )
            except Exception as e:

            enhanced_engine = EnhancedLegalSearchEngine()
            web_engine = ComprehensiveWebSearchEngine()
            queries = enhanced_engine.generate_enhanced_legal_queries(citation)
            all_results = []
            seen_urls = set()
            max_results = 40
            error_threshold = 2
            query_limit = 3
            global_timeout = 15  # seconds

            cluster_citations = [citation]
            all_variants = set()
            for cit in cluster_citations:
                all_variants.add(cit.strip())
                all_variants.add(enhanced_engine.normalize_citation(cit))
                if enhanced_engine.is_washington_citation(cit):
                    all_variants.update(enhanced_engine.generate_washington_variants(cit))
            or_query = ' OR '.join([f'"{v}"' for v in all_variants if v])
            batched_queries = [{'query': or_query, 'priority': 0, 'type': 'batched_cluster', 'citation': citation}]
            queries = batched_queries + queries

            site_specific_engines = ['casetext', 'leagle', 'casemine', 'justia', 'findlaw', 'openjurist', 'vlex']
            error_counts = {engine: 0 for engine in site_specific_engines}
            found_valid_result = None
            def run_site_query(engine_name, query):
                if error_counts[engine_name] >= error_threshold:
                    return []
                try:
                    search_method = getattr(web_engine, f'search_{engine_name}', None)
                    if search_method is None:
                        return []
                    import asyncio
                    results = asyncio.run(search_method(query, None))
                    time.sleep(1.5)
                    if isinstance(results, dict):
                        results = [results]
                    return results
                except Exception as e:
                    error_counts[engine_name] += 1
                    return []
            with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
                future_to_info = {}
                for query_info in queries[:query_limit]:
                    query = query_info['query']
                    for engine_name in site_specific_engines:
                        if error_counts[engine_name] < error_threshold:
                            future = executor.submit(run_site_query, engine_name, query)
                            future_to_info[future] = (engine_name, query)
                start_time = time.time()
                try:
                    for future in concurrent.futures.as_completed(future_to_info, timeout=global_timeout):
                        engine_name, query = future_to_info[future]
                        results = future.result()
                        if not results:
                            continue
                        for result in results:
                            url = result.get('url')
                            if url and url not in seen_urls:
                                seen_urls.add(url)
                                all_results.append(result)
                                legal_results = enhanced_engine.filter_legal_results([result])
                                if legal_results:
                                    best = legal_results[0]
                                    candidate_name = best.get('title')
                                    is_valid = candidate_name and is_valid_case_name(candidate_name)
                                    if is_valid:
                                        found_valid_result = best
                                        break
                            if len(all_results) >= max_results or found_valid_result:
                                break
                        if len(all_results) >= max_results or found_valid_result:
                            break
                        if time.time() - start_time > global_timeout:
                    if found_valid_result:
                        pass
                except concurrent.futures.TimeoutError:
                finally:
                    pass
            if found_valid_result:
                candidate_name = found_valid_result.get('title')
                url = found_valid_result.get('url')
                domain = None
                if url:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc.lower()
                if not domain:
                    domain = found_valid_result.get('source', 'Web Search')
                return CanonicalResult(
                    case_name=candidate_name,
                    date=None,  # Date extraction can be added if available
                    court=None,
                    docket_number=None,
                    url=url,
                    source=domain,
                    confidence=found_valid_result.get('legal_relevance_score', 0) / 100.0,
                    verified=found_valid_result.get('legal_relevance_score', 0) >= 70
                )

            engines = ['google', 'bing', 'ddg']
            error_counts = {engine: 0 for engine in engines}
            found_valid_result = None
            def run_query(engine_name, query):
                if error_counts[engine_name] >= error_threshold:
                    return []
                try:
                    results = web_engine.search_with_engine(query, engine_name, num_results=20)
                    time.sleep(1.5)
                    return results
                except Exception as e:
                    error_counts[engine_name] += 1
                    return []
            with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
                future_to_info = {}
                for query_info in queries[:query_limit]:
                    query = query_info['query']
                    for engine_name in engines:
                        if error_counts[engine_name] < error_threshold:
                            future = executor.submit(run_query, engine_name, query)
                            future_to_info[future] = (engine_name, query)
                start_time = time.time()
                try:
                    for future in concurrent.futures.as_completed(future_to_info, timeout=global_timeout):
                        engine_name, query = future_to_info[future]
                        results = future.result()
                        if not results:
                            continue
                        for result in results:
                            url = result.get('url')
                            if url and url not in seen_urls:
                                seen_urls.add(url)
                                all_results.append(result)
                                legal_results = enhanced_engine.filter_legal_results([result])
                                if legal_results:
                                    best = legal_results[0]
                                    candidate_name = best.get('title')
                                    is_valid = candidate_name and is_valid_case_name(candidate_name)
                                    if is_valid:
                                        found_valid_result = best
                                        break
                            if len(all_results) >= max_results or found_valid_result:
                                break
                        if len(all_results) >= max_results or found_valid_result:
                            break
                        if time.time() - start_time > global_timeout:
                    if found_valid_result:
                        pass
                except concurrent.futures.TimeoutError:
                finally:
                    pass
            if found_valid_result:
                candidate_name = found_valid_result.get('title')
                url = found_valid_result.get('url')
                domain = None
                if url:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc.lower()
                if not domain:
                    domain = found_valid_result.get('source', 'Web Search')
                return CanonicalResult(
                    case_name=candidate_name,
                    date=None,  # Date extraction can be added if available
                    court=None,
                    docket_number=None,
                    url=url,
                    source=domain,
                    confidence=found_valid_result.get('legal_relevance_score', 0) / 100.0,
                    verified=found_valid_result.get('legal_relevance_score', 0) >= 70
                )

            # TODO: Integrate Google Custom Search API here if/when available

            legal_results = enhanced_engine.filter_legal_results(all_results)
            for res in legal_results[:5]:
                print(f"  Score: {res.get('legal_relevance_score', 0)} | Title: {res.get('title')} | URL: {res.get('url')}")

            if legal_results:
                best = legal_results[0]
                
                if len(legal_results) > 1 and legal_results[0].get('legal_relevance_score', 0) >= 80:
                    try:
                        from src.enhanced_case_name_matcher import enhanced_matcher
                        
                        candidate_names = [res.get('title', '') for res in legal_results[:3] if res.get('title')]
                        
                        if citation:
                            citation_words = citation.split()
                            if len(citation_words) >= 3:
                                potential_case_name = ' '.join(citation_words[:3])  # Simple heuristic
                                
                                best_match = enhanced_matcher.find_best_match(
                                    potential_case_name, candidate_names, threshold=0.4
                                )
                                if best_match:
                                    best_candidate = best_match[0]
                                    for res in legal_results:
                                        if res.get('title') == best_candidate:
                                            best = res
                                            break
                    except ImportError:
                        pass
                
                candidate_name = best.get('title')
                is_valid = candidate_name and is_valid_case_name(candidate_name)
                if not is_valid:
                    return None
                url = best.get('url')
                domain = None
                if url:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc.lower()
                if not domain:
                    domain = best.get('source', 'Web Search')
                # that this citation belongs to that case based on case name matching
                
                return CanonicalResult(
                    case_name=candidate_name,
                    date=None,  # Date extraction can be added if available
                    court=None,
                    docket_number=None,
                    url=url,
                    source=domain,
                    confidence=best.get('legal_relevance_score', 0) / 100.0,
                    verified=best.get('legal_relevance_score', 0) >= 70
                )
            else:
        except Exception as e:
            logger.error(f"Web search lookup failed: {e}")
            import traceback
            logger.error(f"Exception traceback: {traceback.format_exc()}")
        return None

def is_valid_case_name(name: Optional[str]) -> bool:
    """Return True if the name looks like a real legal case name (e.g., contains ' v. ' or ' vs. ')."""
    if not name or not isinstance(name, str):
        return False
    import re
    if not re.search(r"\s(v\.|vs\.|v|vs)\s", name, re.IGNORECASE):
        return False
    if re.search(r"(\.com|\.net|\.org|https?://|store|watch|eventify|app|allareacodes|youtube|steampowered|outlook)", name, re.IGNORECASE):
        return False
    if len(name.strip()) < 7:
        return False
    return True

_canonical_service = None

def get_canonical_case_name_with_date(citation: Optional[str], api_key: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    Main function to get canonical case name and date information.
    
    Args:
        citation: The citation string to look up
        api_key: Optional API key (for backward compatibility)
        
    Returns:
        Dictionary with case_name, date, court, etc. or None if not found
    """
    global _canonical_service
    
    if citation is None:
        citation = ""
    
    if _canonical_service is None:
        _canonical_service = CanonicalCaseNameService()
    
    try:
        citation_str: str = citation
        normalized_citation = _canonical_service._normalize_citation_comprehensive(str(citation_str), purpose="verification")
        logger.info(f"get_canonical_case_name_with_date called for: {citation_str} (normalized: {normalized_citation})")
        
        result = _canonical_service._cached_lookup(normalized_citation)
        logger.info(f"get_canonical_case_name_with_date result: {result}")
        
        from src.canonical_case_name_service import is_valid_case_name
        if result and result.case_name is not None:
            if not is_valid_case_name(result.case_name):
                logger.warning(f"[TOP-HARDFILTER] Rejected result for '{citation_str}': '{result.case_name}' is not a valid case name (top-level hard filter)")
                return None
            else:
                logger.info(f"[TOP-HARDFILTER] Accepted result for '{citation_str}': '{result.case_name}' is a valid case name (top-level hard filter)")
        
        if result:
            return {
                'case_name': result.case_name,
                'date': result.date,
                'court': result.court,
                'docket_number': result.docket_number,
                'url': result.url,
                'source': result.source,
                'confidence': result.confidence,
                'verified': result.verified,
                'parallel_citations': result.parallel_citations
            }
        
        logger.info(f"No canonical result found for: {citation_str}")
        return None
        
    except Exception as e:
        logger.error(f"Error in get_canonical_case_name_with_date: {e}")
        return None

def get_canonical_case_name(citation: Optional[str], api_key: Optional[str] = None) -> Optional[str]:
    """
    Simplified function to get just the case name.
    
    Args:
        citation: The citation string to look up
        api_key: Optional API key (for backward compatibility)
        
    Returns:
        Case name string or None if not found
    """
    if citation is None:
        citation = ""
    citation_str: str = citation
    result = get_canonical_case_name_with_date(citation_str, api_key)
    return result.get('case_name') if result else None

def test_canonical_lookup():
    """Test function for canonical lookup"""
    import logging
    logger = logging.getLogger(__name__)
    
    test_citations = [
        "171 Wn.2d 486",
        "200 Wn.2d 72", 
        "514 P.3d 643",
        "410 U.S. 113",  # Roe v. Wade
        "347 U.S. 483"   # Brown v. Board
    ]
    
    logger.info("=== Testing Canonical Case Name Lookup ===")
    
    for citation in test_citations:
        logger.info(f"Testing: {citation}")
        result = get_canonical_case_name_with_date(citation)
        
        if result:
            logger.info(f"  ✅ Found: {result['case_name']}")
            logger.info(f"     Date: {result['date']}")
            logger.info(f"     Court: {result['court']}")
            logger.info(f"     Source: {result['source']}")
            logger.info(f"     Confidence: {result['confidence']}")
        else:
            logger.info(f"  ❌ Not found")

if __name__ == "__main__":
    test_canonical_lookup() 