
"""
Machine Learning Citation Classifier

This module provides functionality to train and use a machine learning model
to classify citations as reliable or unreliable based on patterns in their format,
content, and context. This reduces the need for API calls to verify citations.
"""

import osfrom src.config import DEFAULT_REQUEST_TIMEOUT, COURTLISTENER_TIMEOUT, CASEMINE_TIMEOUT, WEBSEARCH_TIMEOUT, SCRAPINGBEE_TIMEOUT

import re
import json
import pickle
import logging
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

logger = logging.getLogger(__name__)

DOWNLOAD_DIR = "downloaded_briefs"
UNCONFIRMED_CITATIONS_FILE = os.path.join(
    DOWNLOAD_DIR, "unconfirmed_citations_flat.json"
)
MODEL_FILE = os.path.join(DOWNLOAD_DIR, "citation_classifier_model.pkl")
VECTORIZER_FILE = os.path.join(DOWNLOAD_DIR, "citation_vectorizer.pkl")


def extract_features_from_citation(citation_text, canonical_name=None):
    """Extract features from citation text for classification."""
    features = {}
    
    features["length"] = len(citation_text)
    features["has_case_name"] = 1 if canonical_name and len(canonical_name) > 0 else 0
    if canonical_name:
        features["canonical_name_length"] = len(canonical_name)
    else:
        features["canonical_name_length"] = 0

    features["has_volume_reporter_page"] = (
        1 if re.search(r"\d+\s+[A-Za-z\.]+\s+\d+", citation_text) else 0
    )
    features["has_year"] = 1 if re.search(r"\(\d{4}\)", citation_text) else 0
    features["has_court_abbreviation"] = (
        1
        if re.search(
            r"\b(S\.\s*Ct\.|F\.\s*\d+d|U\.\s*S\.|P\.\s*\d+d|Wn\.\s*\d+d|Wn\.\s*App\.)",
            citation_text,
            re.IGNORECASE,
        )
        else 0
    )

    features["is_wl_citation"] = (
        1 if re.search(r"\d{4}\s+WL\s+\d+", citation_text) else 0
    )

    features["has_unusual_characters"] = (
        1 if re.search(r"[^\w\s\.,;\(\)\[\]\-]", citation_text) else 0
    )
    features["has_excessive_numbers"] = (
        1 if len(re.findall(r"\d+", citation_text)) > 5 else 0
    )

    reporter_match = re.search(r"([A-Za-z]+\.(?:\s*\d+)?[A-Za-z]*)", citation_text)
    features["has_valid_reporter"] = 0

    if reporter_match:
        reporter = reporter_match.group(1).lower()
        common_reporters = [
            "f.",
            "f.2d",
            "f.3d",
            "u.s.",
            "s.ct.",
            "l.ed.",
            "p.",
            "p.2d",
            "p.3d",
            "wn.",
            "wn.2d",
            "wn.app.",
            "wash.",
            "wash.2d",
            "wash.app.",
        ]
        features["has_valid_reporter"] = (
            1 if any(r in reporter for r in common_reporters) else 0
        )

    return features


def prepare_citation_data(citations):
    """Prepare citation data for training the model."""
    data = []
    labels = []
    texts = []

    for citation in citations:
        if "confidence" not in citation:
            continue

        features = extract_features_from_citation(
            citation.get("citation_text", ""), citation.get("canonical_name", "")
        )

        label = 1 if citation.get("confidence", 0) >= 0.7 else 0

        data.append(features)
        labels.append(label)
        texts.append(citation.get("citation_text", ""))

    return pd.DataFrame(data), np.array(labels), texts


def train_citation_classifier():
    """Train a machine learning model to classify citations."""
    logger.info("Loading citation data...")
    try:
        with open(UNCONFIRMED_CITATIONS_FILE, "r", encoding="utf-8") as f:
            citations = json.load(f)
    except Exception as e:
        logger.error(f"Error loading citation data: {e}")
        return None, None

    if len(citations) < 50:
        logger.info(f"Not enough citation data to train a model. Found only {len(citations)} citations."
        )
        return None, None

    logger.info(f"Preparing data from {len(citations)} citations...")
    features_df, labels, texts = prepare_citation_data(citations)

    logger.info("Creating text features...")
    vectorizer = TfidfVectorizer(
        analyzer="char_wb", ngram_range=(2, 5), max_features=200
    )
    vectorizer.fit_transform(texts)

    feature_array = features_df.to_numpy()

    X_train, X_test, y_train, y_test, texts_train, texts_test = train_test_split(
        feature_array, labels, texts, test_size=0.2, random_state=42
    )

    X_text_train = vectorizer.transform(texts_train)
    X_text_test = vectorizer.transform(texts_test)

    logger.info("Training the classifier...")
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_text_train, y_train)

    y_pred = model.predict(X_text_test)
    accuracy = accuracy_score(y_test, y_pred)
    logger.info(f"Model accuracy: {accuracy:.2f}")
    logger.info(classification_report(y_test, y_pred))

    logger.info("Saving model and vectorizer...")
    with open(MODEL_FILE, "wb") as f:
        pickle.dump(model, f)

    with open(VECTORIZER_FILE, "wb") as f:
        pickle.dump(vectorizer, f)

    logger.info(f"Model and vectorizer saved to {DOWNLOAD_DIR}")
    return model, vectorizer


def load_citation_classifier():
    """Load the trained citation classifier model."""
    try:
        if not os.path.exists(MODEL_FILE) or not os.path.exists(VECTORIZER_FILE):
            logger.info("Model or vectorizer file not found. Training new model...")
            return train_citation_classifier()

        with open(MODEL_FILE, "rb") as f:
            model = pickle.load(f)

        with open(VECTORIZER_FILE, "rb") as f:
            vectorizer = pickle.load(f)

        logger.info("Citation classifier model loaded successfully.")
        return model, vectorizer
    except Exception as e:
        logger.error(f"Error loading citation classifier: {e}")
        return None, None


def classify_citation(citation_text, canonical_name=None):
    """
    Classify a citation as reliable or unreliable using the trained model.
    Returns a confidence score between 0.0 and 1.0.
    """
    model, vectorizer = load_citation_classifier()

    if model is None or vectorizer is None:
        logger.info("Could not load or train the citation classifier model.")
        return 0.5  # Return neutral confidence if model is unavailable

    try:
        features = extract_features_from_citation(citation_text, canonical_name)

        text_features = vectorizer.transform([citation_text])

        try:
            proba = model.predict_proba(text_features)

            confidence = 0.5

            if hasattr(proba, "shape"):
                if len(proba.shape) > 1 and proba.shape[0] > 0:
                    if proba.shape[1] > 1:
                        confidence = float(
                            proba[0, 1]
                        )  # Probability of reliable class (index 1)
                    else:
                        confidence = float(proba[0, 0])
                elif len(proba.shape) == 1 and len(proba) > 0:
                    confidence = float(proba[0])
            elif isinstance(proba, (list, np.ndarray)) and len(proba) > 0:
                if len(proba) > 1:
                    confidence = float(proba[-1])
                else:
                    confidence = float(proba[0])
            else:
                prediction = model.predict(text_features)
                confidence = (
                    float(prediction[0])
                    if hasattr(prediction, "__len__") and len(prediction) > 0
                    else 0.5
                )
        except (IndexError, TypeError, AttributeError) as e:
            logger.error(f"Error processing prediction probabilities: {e}")
            try:
                prediction = model.predict(text_features)
                confidence = (
                    float(prediction[0])
                    if hasattr(prediction, "__len__") and len(prediction) > 0
                    else 0.5
                )
            except Exception as e2:
                logger.error(f"Fallback prediction also failed: {e2}")
                confidence = 0.5

        if features["has_valid_reporter"] == 0:
            confidence *= 0.8  # Reduce confidence if no valid reporter

        if features["is_wl_citation"] == 1:
            confidence *= 0.9  # Slightly reduce confidence for WL citations

        if features["has_unusual_characters"] == 1:
            confidence *= 0.7  # Reduce confidence for unusual characters

        return confidence
    except Exception as e:
        logger.error(f"Error classifying citation: {e}")
        return 0.5  # Return neutral confidence on error


def batch_classify_citations(citations):
    """
    Classify multiple citations and return confidence scores.
    """
    model, vectorizer = load_citation_classifier()

    if model is None or vectorizer is None:
        logger.info("Could not load or train the citation classifier model.")
        return [0.5] * len(citations)  # Return neutral confidence for all

    try:
        texts = [c.get("citation_text", "") for c in citations]
        if not texts:  # Handle empty input
            return []

        text_features = vectorizer.transform(texts)

        try:
            probas = model.predict_proba(text_features)

            confidences = [0.5] * len(texts)

            confidences = [0.5] * len(texts)  # Initialize with default confidence

            try:
                if hasattr(probas, "shape"):
                    if len(probas.shape) == 2 and probas.shape[0] > 0:
                        if probas.shape[1] > 1:
                            confidences = [
                                float(p[-1]) for p in probas
                            ]  # Take last probability
                        else:
                            confidences = [float(p[0]) for p in probas]
                    elif len(probas.shape) == 1:
                        confidences = [float(p) for p in probas]
                elif isinstance(probas, (list, np.ndarray)) and len(probas) > 0:
                    confidences = []
                    for p in probas:
                        try:
                            if hasattr(p, "__len__") and len(p) > 1:
                                confidences.append(float(p[-1]))
                            else:
                                confidences.append(
                                    float(p[0])
                                    if hasattr(p, "__len__") and len(p) > 0
                                    else 0.5
                                )
                        except (IndexError, TypeError, ValueError) as e:
                            logger.error(f"Error processing probability: {e}")
                            confidences.append(0.5)
            except Exception as e:
                logger.error(f"Error processing batch probabilities: {e}")
                try:
                    predictions = model.predict(text_features)
                    confidences = (
                        [float(p) for p in predictions]
                        if hasattr(predictions, "__len__")
                        else [0.5] * len(texts)
                    )
                except Exception as e2:
                    logger.error(f"Fallback prediction also failed: {e2}")
                    confidences = [0.5] * len(texts)
        except Exception as e:
            logger.error(f"Error in batch prediction: {e}")
            confidences = [0.5] * len(texts)

        for i, citation in enumerate(citations):
            try:
                features = extract_features_from_citation(
                    citation.get("citation_text", ""), citation.get("canonical_name", "")
                )

                if features["has_valid_reporter"] == 0:
                    confidences[i] *= 0.8  # Reduce confidence if no valid reporter

                if features["is_wl_citation"] == 1:
                    confidences[i] *= 0.9  # Slightly reduce confidence for WL citations

                if features["has_unusual_characters"] == 1:
                    confidences[i] *= 0.7  # Reduce confidence for unusual characters
            except Exception as e:
                logger.error(f"Error processing citation features: {e}")

        return confidences
    except Exception as e:
        logger.error(f"Error batch classifying citations: {e}")
        return [0.5] * len(citations)  # Return neutral confidence on error


if __name__ == "__main__":
    logger.info("Training citation classifier...")
    train_citation_classifier()

    test_citations = [
        "550 U.S. 544 (2007)",  # Should be high confidence
        "123 Fake Reporter 456",  # Should be low confidence
        "2020 WL 123456",  # Should be medium confidence
        "Wn.2d 123, 456 P.3d 789 (2019)",  # Should be high confidence
    ]

    logger.info("\nTesting classifier on examples:")
    for citation in test_citations:
        confidence = classify_citation(citation)
        logger.info(f"Citation: {citation}")
        logger.info(f"Confidence: {confidence:.2f}")
        logger.info(f"Classification: {'Reliable' if confidence >= 0.7 else 'Unreliable'}")
        logger.info("")  # Empty line for readability
