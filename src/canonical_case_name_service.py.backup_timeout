"""
Canonical Case Name and Date Retrieval Implementation
Provides authoritative case information from multiple legal databases
"""

import re
import logging
import requests
import time
from typing import Dict, Optional, Any, List
from urllib.parse import quote_plus
import json
from functools import lru_cache
from dataclasses import dataclass
try:
    from .config import get_config_value
except ImportError:
    from config import get_config_value
import os
from src.comprehensive_websearch_engine import search_cluster_for_canonical_sources
import concurrent.futures
import sys

logger = logging.getLogger(__name__)

fallback_logger = logging.getLogger('fallback_usage')
fallback_logger.setLevel(logging.INFO)

try:
    logs_dir = 'logs'
    if not os.path.exists(logs_dir):
        os.makedirs(logs_dir, exist_ok=True)
    fallback_log_path = os.path.join(logs_dir, 'fallback_usage.log')
    fallback_handler = logging.FileHandler(fallback_log_path)
    fallback_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    ))
    fallback_logger.addHandler(fallback_handler)
except Exception as e:
    logger.warning(f"Could not create fallback log file: {e}")
    logger.error(f"FALLBACK LOGGING ERROR: Could not create fallback log file: {e}")

def log_fallback_usage(citation: str, fallback_type: str, reason: str, context: Optional[Dict] = None):
    """Log fallback usage to dedicated log file"""
    try:
        log_entry = {
            'timestamp': time.time(),
            'citation': citation,
            'fallback_type': fallback_type,
            'reason': reason,
            'context': context or {}
        }
        fallback_logger.info(f"FALLBACK_USED: {json.dumps(log_entry)}")
    except Exception as e:
        logger.error(f"Failed to log fallback usage: {e}")

@dataclass
class CanonicalResult:
    """Structured result for canonical case information"""
    case_name: Optional[str] = None
    date: Optional[str] = None
    court: Optional[str] = None
    docket_number: Optional[str] = None
    url: Optional[str] = None
    source: str = "unknown"
    confidence: float = 0.0
    verified: bool = False
    parallel_citations: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.parallel_citations is None:
            self.parallel_citations = []

class CanonicalCaseNameService:
    """
    Service for retrieving canonical case names and dates from legal databases
    """
    
    def __init__(self):
        self.courtlistener_api_key = self._get_config_value("COURTLISTENER_API_KEY")
        self.caselaw_api_key = self._get_config_value("CASELAW_API_KEY")
        self.westlaw_api_key = self._get_config_value("WESTLAW_API_KEY")
        
        self.last_request_time = {}
        self.min_request_interval = 1.0  # seconds between requests
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'CaseStrainer/2.0 Legal Citation Processor'
        })
        
        self.citation_patterns = {
            'wn2d': r'(\d+)\s+Wn\.2d\s+(\d+)',
            'wn_app': r'(\d+)\s+Wn\.\s*App\.\s+(\d+)', 
            'p3d': r'(\d+)\s+P\.3d\s+(\d+)',
            'p2d': r'(\d+)\s+P\.2d\s+(\d+)',
            'us': r'(\d+)\s+U\.S\.\s+(\d+)',
            'f3d': r'(\d+)\s+F\.3d\s+(\d+)',
            'f2d': r'(\d+)\s+F\.2d\s+(\d+)',
        }
        
        self._cached_lookup.cache_clear()
    
    def _get_config_value(self, key: str) -> Optional[str]:
        """Get configuration value from environment or config file"""
        import os
        try:
            value = os.environ.get(key)
            if value:
                return value
            
            return get_config_value(key)
        except ImportError:
            return os.environ.get(key)
    
    def _normalize_citation_comprehensive(self, citation: str, purpose: str = "verification") -> str:
        """
        Normalize citation format for better API matching.
        
        Args:
            citation: The citation to normalize
            purpose: The purpose of normalization ("general", "bluebook", "verification", "comparison")
            
        Returns:
            The normalized citation
        """
        if not citation:
            return ""
        
        normalized = re.sub(r'\s+', ' ', citation.strip())
        
        normalized = re.sub(r'Wash\.\s*2d', 'Wn.2d', normalized)
        normalized = re.sub(r'Wash\.\s*App\.', 'Wn. App.', normalized)
        
        normalized = re.sub(r'P\.\s*3d', 'P.3d', normalized)
        normalized = re.sub(r'P\.\s*2d', 'P.2d', normalized)
        
        return normalized
    
    def _extract_citation_components(self, citation: str) -> Dict[str, str]:
        """Extract volume, reporter, and page from citation"""
        components = {'volume': '', 'reporter': '', 'page': ''}
        
        for pattern_name, pattern in self.citation_patterns.items():
            match = re.search(pattern, citation)
            if match:
                components['volume'] = match.group(1)
                components['page'] = match.group(2)
                
                reporter_map = {
                    'wn2d': 'Wn.2d',
                    'wn_app': 'Wn. App.',
                    'p3d': 'P.3d',
                    'p2d': 'P.2d',
                    'us': 'U.S.',
                    'f3d': 'F.3d',
                    'f2d': 'F.2d'
                }
                components['reporter'] = reporter_map.get(pattern_name, '')
                break
        
        return components
    
    def _rate_limit(self, service: str):
        """Implement rate limiting for API requests"""
        now = time.time()
        last_time = self.last_request_time.get(service, 0)
        
        time_since_last = now - last_time
        if time_since_last < self.min_request_interval:
            sleep_time = self.min_request_interval - time_since_last
            time.sleep(sleep_time)
        
        self.last_request_time[service] = time.time()
    
    @lru_cache(maxsize=1000)
    def _cached_lookup(self, normalized_citation: str) -> Optional[CanonicalResult]:
        """Cached lookup to avoid repeated API calls"""
        return self._perform_lookup(normalized_citation)
    
    def _perform_lookup(self, citation: str) -> Optional[CanonicalResult]:
        """Perform the actual lookup across multiple services"""
        
        services = [
            ('courtlistener', self._lookup_courtlistener),
            ('web_sources', self._lookup_web_sources)   # Web search (can target CAP, Westlaw, etc.)
        ]
        
        failed_services = []
        for service_name, lookup_func in services:
            try:
                self._rate_limit(service_name)
                result = lookup_func(citation)
                from src.canonical_case_name_service import is_valid_case_name
                if result and result.case_name:
                    if not is_valid_case_name(result.case_name):
                        logger.warning(f"[HARDFILTER] Rejected result from {service_name} for '{citation or ''}': '{result.case_name}' is not a valid case name (hard filter)")
                        continue
                    else:
                        logger.info(f"[HARDFILTER] Accepted result from {service_name} for '{citation or ''}': '{result.case_name}' is a valid case name (hard filter)")
                if result and result.case_name:
                    logger.info(f"Found canonical result via {service_name}: {result.case_name}")
                    if service_name == 'web_sources':
                        log_fallback_usage(
                            citation=citation,
                            fallback_type='canonical_lookup',
                            reason=f"Primary service (courtlistener) failed, using {service_name}",
                            context={'result_case_name': result.case_name, 'result_source': result.source}
                        )
                        result.source = f"fallback: {result.source}"
                    return result
                else:
                    failed_services.append(f"{service_name}(no_result)")
            except Exception as e:
                failed_services.append(f"{service_name}(error:{str(e)})")
                logger.warning(f"Lookup failed for {service_name}: {e}")
                continue
        
        log_fallback_usage(
            citation=citation,
            fallback_type='canonical_lookup',
            reason=f"All services failed: {', '.join(failed_services)}",
            context={'failed_services': failed_services}
        )
        return None
    
    def _lookup_courtlistener(self, citation: str) -> Optional[CanonicalResult]:
        """Lookup using CourtListener API"""
        if not self.courtlistener_api_key:
            return None
        
        try:
            components = self._extract_citation_components(citation)
            if not all(components.values()):
                return None
            
            citation_text = f'{components["volume"]} {components["reporter"]} {components["page"]}'
            query = f'citation:"{citation_text}"'  # Search specifically in citation field
            
            url = "https://www.courtlistener.com/api/rest/v4/search/"
            headers = {"Authorization": f"Token {self.courtlistener_api_key}"}
            params = {
                "q": query,
                "type": "o",  # Opinions
                "format": "json",
                "page_size": 10,
                "order_by": "score desc"
            }
            
            response = self.session.get(url, headers=headers, params=params, timeout=5)
            response.raise_for_status()
            
            data = response.json()
            results = data.get('results', [])
            
            for result in results:
                citation_string = result.get('citation', [])
                if isinstance(citation_string, list):
                    citation_string = ' '.join(citation_string)
                
                if citation.lower() in str(citation_string).lower():
                    return CanonicalResult(
                        case_name=result.get('caseName', ''),
                        date=result.get('dateFiled', ''),
                        court=result.get('court', ''),
                        docket_number=result.get('docketNumber', ''),
                        url=result.get('absolute_url', ''),
                        source='CourtListener',
                        confidence=0.9,
                        verified=True,
                        parallel_citations=result.get('citation', [])
                    )
                
                if (components['volume'] in str(citation_string) and 
                    components['page'] in str(citation_string) and
                    components['reporter'] in str(citation_string)):
                    
                    return CanonicalResult(
                        case_name=result.get('caseName', ''),
                        date=result.get('dateFiled', ''),
                        court=result.get('court', ''),
                        docket_number=result.get('docketNumber', ''),
                        url=result.get('absolute_url', ''),
                        source='CourtListener',
                        confidence=0.9,
                        verified=True,
                        parallel_citations=result.get('citation', [])
                    )
            
            return None
            
        except Exception as e:
            logger.error(f"CourtListener lookup failed: {e}")
            return None
    
    def _lookup_caselaw_access(self, citation: str) -> Optional[CanonicalResult]:
        """Lookup using Caselaw Access Project API"""
        try:
            url = "https://api.case.law/v1/cases/"
            params = {
                "cite": citation,
                "format": "json",
                "full_case": "false"
            }
            
            if self.caselaw_api_key:
                headers = {"Authorization": f"Token {self.caselaw_api_key}"}
            else:
                headers = {}
            
            response = self.session.get(url, headers=headers, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            results = data.get('results', [])
            
            if results:
                result = results[0]  # Take first result
                
                date = result.get('decision_date', '')
                if date:
                    year_match = re.search(r'(\d{4})', date)
                    if year_match:
                        date = year_match.group(1)
                
                return CanonicalResult(
                    case_name=result.get('name_abbreviation', ''),
                    date=date,
                    court=result.get('court', {}).get('name', ''),
                    docket_number=result.get('docket_number', ''),
                    url=result.get('frontend_url', ''),
                    source='Caselaw Access Project',
                    confidence=0.85,
                    verified=True,
                    parallel_citations=result.get('citations', [])
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Caselaw Access lookup failed: {e}")
            return None
    
    def _lookup_westlaw(self, citation: str) -> Optional[CanonicalResult]:
        """Lookup using Westlaw API or fallback to canonical verification."""
        try:
            try:
                from src.unified_citation_processor_v2 import UnifiedCitationProcessorV2
                PROCESSOR_AVAILABLE = True
            except ImportError:
                PROCESSOR_AVAILABLE = False
                logger.warning("UnifiedCitationProcessorV2 not available")
                
            if PROCESSOR_AVAILABLE:
                processor = UnifiedCitationProcessorV2()
                try:
                    from src.models import CitationResult
                    citation_result = CitationResult(citation=citation)
                    verified = processor._verify_citation_with_courtlistener(citation_result)
                    if verified:
                        return CanonicalResult(
                            case_name=citation_result.canonical_name,
                            date=citation_result.canonical_date,
                            url=citation_result.url,
                            source="Westlaw (via UnifiedProcessor)",
                            confidence=0.8
                        )
                except Exception as e:
                    logger.warning(f"Failed to verify citation {citation or ''}: {e}")
            
            if not self.westlaw_api_key:
                return None
            
            url = "https://api.westlaw.com/v1/cases"
            headers = {
                "Authorization": f"Bearer {self.westlaw_api_key}",
                "Content-Type": "application/json"
            }
            params = {"q": citation}
            
            response = self.session.get(url, headers=headers, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            return None
            
        except Exception as e:
            logger.error(f"Westlaw lookup failed: {e}")
            return None
    
    def _lookup_web_sources(self, citation: str) -> Optional[CanonicalResult]:
        """Lookup using intelligent web scraping with EnhancedLegalSearchEngine and multiple engines, with cluster batching and a global timeout. Site-specific legal DBs are tried before general search engines."""
        try:
            import logging
            import asyncio
            import time
            from src.enhanced_legal_search_engine import EnhancedLegalSearchEngine
            from src.comprehensive_websearch_engine import ComprehensiveWebSearchEngine
            logger = logging.getLogger(__name__)

            import subprocess
            import sys
            try:
                result = subprocess.run([
                    sys.executable, "playwright_search_justia.py", citation
                ], capture_output=True, text=True, timeout=12)
                output = result.stdout.strip()
                if "Case name:" in output:
                    case_name = output.split("Case name:")[-1].strip()
                    if case_name and case_name.lower() != 'none':
                        return CanonicalResult(
                            case_name=case_name,
                            date=None,
                            url=None,
                            source="justia (playwright)",
                            confidence=0.85,
                            verified=True
                        )
            except Exception as e:

            enhanced_engine = EnhancedLegalSearchEngine()
            web_engine = ComprehensiveWebSearchEngine()
            queries = enhanced_engine.generate_enhanced_legal_queries(citation)
            all_results = []
            seen_urls = set()
            max_results = 40
            error_threshold = 2
            query_limit = 3
            global_timeout = 15  # seconds

            cluster_citations = [citation]
            all_variants = set()
            for cit in cluster_citations:
                all_variants.add(cit.strip())
                all_variants.add(enhanced_engine.normalize_citation(cit))
                if enhanced_engine.is_washington_citation(cit):
                    all_variants.update(enhanced_engine.generate_washington_variants(cit))
            or_query = ' OR '.join([f'"{v}"' for v in all_variants if v])
            batched_queries = [{'query': or_query, 'priority': 0, 'type': 'batched_cluster', 'citation': citation}]
            queries = batched_queries + queries

            site_specific_engines = ['casetext', 'leagle', 'casemine', 'justia', 'findlaw', 'openjurist', 'vlex']
            error_counts = {engine: 0 for engine in site_specific_engines}
            found_valid_result = None
            def run_site_query(engine_name, query):
                if error_counts[engine_name] >= error_threshold:
                    return []
                try:
                    search_method = getattr(web_engine, f'search_{engine_name}', None)
                    if search_method is None:
                        return []
                    import asyncio
                    results = asyncio.run(search_method(query, None))
                    time.sleep(1.5)
                    if isinstance(results, dict):
                        results = [results]
                    return results
                except Exception as e:
                    error_counts[engine_name] += 1
                    return []
            with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
                future_to_info = {}
                for query_info in queries[:query_limit]:
                    query = query_info['query']
                    for engine_name in site_specific_engines:
                        if error_counts[engine_name] < error_threshold:
                            future = executor.submit(run_site_query, engine_name, query)
                            future_to_info[future] = (engine_name, query)
                start_time = time.time()
                try:
                    for future in concurrent.futures.as_completed(future_to_info, timeout=global_timeout):
                        engine_name, query = future_to_info[future]
                        results = future.result()
                        if not results:
                            continue
                        for result in results:
                            url = result.get('url')
                            if url and url not in seen_urls:
                                seen_urls.add(url)
                                all_results.append(result)
                                legal_results = enhanced_engine.filter_legal_results([result])
                                if legal_results:
                                    best = legal_results[0]
                                    candidate_name = best.get('title')
                                    is_valid = candidate_name and is_valid_case_name(candidate_name)
                                    if is_valid:
                                        found_valid_result = best
                                        break
                            if len(all_results) >= max_results or found_valid_result:
                                break
                        if len(all_results) >= max_results or found_valid_result:
                            break
                        if time.time() - start_time > global_timeout:
                    if found_valid_result:
                        pass
                except concurrent.futures.TimeoutError:
                finally:
                    pass
            if found_valid_result:
                candidate_name = found_valid_result.get('title')
                url = found_valid_result.get('url')
                domain = None
                if url:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc.lower()
                if not domain:
                    domain = found_valid_result.get('source', 'Web Search')
                return CanonicalResult(
                    case_name=candidate_name,
                    date=None,  # Date extraction can be added if available
                    court=None,
                    docket_number=None,
                    url=url,
                    source=domain,
                    confidence=found_valid_result.get('legal_relevance_score', 0) / 100.0,
                    verified=found_valid_result.get('legal_relevance_score', 0) >= 70
                )

            engines = ['google', 'bing', 'ddg']
            error_counts = {engine: 0 for engine in engines}
            found_valid_result = None
            def run_query(engine_name, query):
                if error_counts[engine_name] >= error_threshold:
                    return []
                try:
                    results = web_engine.search_with_engine(query, engine_name, num_results=20)
                    time.sleep(1.5)
                    return results
                except Exception as e:
                    error_counts[engine_name] += 1
                    return []
            with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
                future_to_info = {}
                for query_info in queries[:query_limit]:
                    query = query_info['query']
                    for engine_name in engines:
                        if error_counts[engine_name] < error_threshold:
                            future = executor.submit(run_query, engine_name, query)
                            future_to_info[future] = (engine_name, query)
                start_time = time.time()
                try:
                    for future in concurrent.futures.as_completed(future_to_info, timeout=global_timeout):
                        engine_name, query = future_to_info[future]
                        results = future.result()
                        if not results:
                            continue
                        for result in results:
                            url = result.get('url')
                            if url and url not in seen_urls:
                                seen_urls.add(url)
                                all_results.append(result)
                                legal_results = enhanced_engine.filter_legal_results([result])
                                if legal_results:
                                    best = legal_results[0]
                                    candidate_name = best.get('title')
                                    is_valid = candidate_name and is_valid_case_name(candidate_name)
                                    if is_valid:
                                        found_valid_result = best
                                        break
                            if len(all_results) >= max_results or found_valid_result:
                                break
                        if len(all_results) >= max_results or found_valid_result:
                            break
                        if time.time() - start_time > global_timeout:
                    if found_valid_result:
                        pass
                except concurrent.futures.TimeoutError:
                finally:
                    pass
            if found_valid_result:
                candidate_name = found_valid_result.get('title')
                url = found_valid_result.get('url')
                domain = None
                if url:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc.lower()
                if not domain:
                    domain = found_valid_result.get('source', 'Web Search')
                return CanonicalResult(
                    case_name=candidate_name,
                    date=None,  # Date extraction can be added if available
                    court=None,
                    docket_number=None,
                    url=url,
                    source=domain,
                    confidence=found_valid_result.get('legal_relevance_score', 0) / 100.0,
                    verified=found_valid_result.get('legal_relevance_score', 0) >= 70
                )

            # TODO: Integrate Google Custom Search API here if/when available

            legal_results = enhanced_engine.filter_legal_results(all_results)
            for res in legal_results[:5]:
                print(f"  Score: {res.get('legal_relevance_score', 0)} | Title: {res.get('title')} | URL: {res.get('url')}")

            if legal_results:
                best = legal_results[0]
                
                if len(legal_results) > 1 and legal_results[0].get('legal_relevance_score', 0) >= 80:
                    try:
                        from src.enhanced_case_name_matcher import enhanced_matcher
                        
                        candidate_names = [res.get('title', '') for res in legal_results[:3] if res.get('title')]
                        
                        if citation:
                            citation_words = citation.split()
                            if len(citation_words) >= 3:
                                potential_case_name = ' '.join(citation_words[:3])  # Simple heuristic
                                
                                best_match = enhanced_matcher.find_best_match(
                                    potential_case_name, candidate_names, threshold=0.4
                                )
                                if best_match:
                                    best_candidate = best_match[0]
                                    for res in legal_results:
                                        if res.get('title') == best_candidate:
                                            best = res
                                            break
                    except ImportError:
                        pass
                
                candidate_name = best.get('title')
                is_valid = candidate_name and is_valid_case_name(candidate_name)
                if not is_valid:
                    return None
                url = best.get('url')
                domain = None
                if url:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc.lower()
                if not domain:
                    domain = best.get('source', 'Web Search')
                # that this citation belongs to that case based on case name matching
                
                return CanonicalResult(
                    case_name=candidate_name,
                    date=None,  # Date extraction can be added if available
                    court=None,
                    docket_number=None,
                    url=url,
                    source=domain,
                    confidence=best.get('legal_relevance_score', 0) / 100.0,
                    verified=best.get('legal_relevance_score', 0) >= 70
                )
            else:
        except Exception as e:
            logger.error(f"Web search lookup failed: {e}")
            import traceback
            logger.error(f"Exception traceback: {traceback.format_exc()}")
        return None

def is_valid_case_name(name: Optional[str]) -> bool:
    """Return True if the name looks like a real legal case name (e.g., contains ' v. ' or ' vs. ')."""
    if not name or not isinstance(name, str):
        return False
    import re
    if not re.search(r"\s(v\.|vs\.|v|vs)\s", name, re.IGNORECASE):
        return False
    if re.search(r"(\.com|\.net|\.org|https?://|store|watch|eventify|app|allareacodes|youtube|steampowered|outlook)", name, re.IGNORECASE):
        return False
    if len(name.strip()) < 7:
        return False
    return True

_canonical_service = None

def get_canonical_case_name_with_date(citation: Optional[str], api_key: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    Main function to get canonical case name and date information.
    
    Args:
        citation: The citation string to look up
        api_key: Optional API key (for backward compatibility)
        
    Returns:
        Dictionary with case_name, date, court, etc. or None if not found
    """
    global _canonical_service
    
    if citation is None:
        citation = ""
    
    if _canonical_service is None:
        _canonical_service = CanonicalCaseNameService()
    
    try:
        citation_str: str = citation
        normalized_citation = _canonical_service._normalize_citation_comprehensive(str(citation_str), purpose="verification")
        logger.info(f"get_canonical_case_name_with_date called for: {citation_str} (normalized: {normalized_citation})")
        
        result = _canonical_service._cached_lookup(normalized_citation)
        logger.info(f"get_canonical_case_name_with_date result: {result}")
        
        from src.canonical_case_name_service import is_valid_case_name
        if result and result.case_name is not None:
            if not is_valid_case_name(result.case_name):
                logger.warning(f"[TOP-HARDFILTER] Rejected result for '{citation_str}': '{result.case_name}' is not a valid case name (top-level hard filter)")
                return None
            else:
                logger.info(f"[TOP-HARDFILTER] Accepted result for '{citation_str}': '{result.case_name}' is a valid case name (top-level hard filter)")
        
        if result:
            return {
                'case_name': result.case_name,
                'date': result.date,
                'court': result.court,
                'docket_number': result.docket_number,
                'url': result.url,
                'source': result.source,
                'confidence': result.confidence,
                'verified': result.verified,
                'parallel_citations': result.parallel_citations
            }
        
        logger.info(f"No canonical result found for: {citation_str}")
        return None
        
    except Exception as e:
        logger.error(f"Error in get_canonical_case_name_with_date: {e}")
        return None

def get_canonical_case_name(citation: Optional[str], api_key: Optional[str] = None) -> Optional[str]:
    """
    Simplified function to get just the case name.
    
    Args:
        citation: The citation string to look up
        api_key: Optional API key (for backward compatibility)
        
    Returns:
        Case name string or None if not found
    """
    if citation is None:
        citation = ""
    citation_str: str = citation
    result = get_canonical_case_name_with_date(citation_str, api_key)
    return result.get('case_name') if result else None

def test_canonical_lookup():
    """Test function for canonical lookup"""
    import logging
    logger = logging.getLogger(__name__)
    
    test_citations = [
        "171 Wn.2d 486",
        "200 Wn.2d 72", 
        "514 P.3d 643",
        "410 U.S. 113",  # Roe v. Wade
        "347 U.S. 483"   # Brown v. Board
    ]
    
    logger.info("=== Testing Canonical Case Name Lookup ===")
    
    for citation in test_citations:
        logger.info(f"Testing: {citation}")
        result = get_canonical_case_name_with_date(citation)
        
        if result:
            logger.info(f"  ✅ Found: {result['case_name']}")
            logger.info(f"     Date: {result['date']}")
            logger.info(f"     Court: {result['court']}")
            logger.info(f"     Source: {result['source']}")
            logger.info(f"     Confidence: {result['confidence']}")
        else:
            logger.info(f"  ❌ Not found")

if __name__ == "__main__":
    test_canonical_lookup() 