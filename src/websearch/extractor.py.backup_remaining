"""
Web Extractor Module
Comprehensive web content extraction for legal documents and search results.
"""

import re
from src.config import DEFAULT_REQUEST_TIMEOUT, COURTLISTENER_TIMEOUT, CASEMINE_TIMEOUT, WEBSEARCH_TIMEOUT, SCRAPINGBEE_TIMEOUT

import logging
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class ComprehensiveWebExtractor:
    """Comprehensive web content extraction for legal documents and search results."""
    
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        self.citation_patterns = {
            'washington': [
                r'(\d+)\s+Wn\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Wn\.?\s*App\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Wash\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Washington\s+(\d+[a-z]?)\s+(\d+)',
            ],
            'federal': [
                r'(\d+)\s+U\.?S\.?\s+(\d+)',
                r'(\d+)\s+F\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+F\.?\s*Supp\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Fed\.?\s*(\d+[a-z]?)\s+(\d+)',
            ],
            'pacific': [
                r'(\d+)\s+P\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Pac\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Pacific\s+(\d+[a-z]?)\s+(\d+)',
            ]
        }
        
        self.case_name_patterns = [
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+v\.\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+vs\.\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+versus\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+et\s+al\.\s+v\.\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
        ]
    
    def generate_washington_variants(self, citation: str) -> List[str]:
        """Generate Washington citation variants."""
        variants = set()
        
        variants.add(citation)
        
        patterns = [
            (r'(\d+)\s+Wn\.?\s*(\d+[a-z]?)\s+(\d+)', r'\1 Wn. \2 \3'),
            (r'(\d+)\s+Wn\.?\s*App\.?\s*(\d+[a-z]?)\s+(\d+)', r'\1 Wn. App. \2 \3'),
            (r'(\d+)\s+Wash\.?\s*(\d+[a-z]?)\s+(\d+)', r'\1 Wash. \2 \3'),
            (r'(\d+)\s+Washington\s+(\d+[a-z]?)\s+(\d+)', r'\1 Washington \2 \3'),
        ]
        
        for pattern, replacement in patterns:
            if re.search(pattern, citation, re.IGNORECASE):
                normalized = re.sub(pattern, replacement, citation, flags=re.IGNORECASE)
                variants.add(normalized)
                
                if 'Wn.' in normalized:
                    variants.add(normalized.replace('Wn.', 'Wash.'))
                    variants.add(normalized.replace('Wn.', 'Washington'))
                elif 'Wash.' in normalized:
                    variants.add(normalized.replace('Wash.', 'Wn.'))
                    variants.add(normalized.replace('Wash.', 'Washington'))
                elif 'Washington' in normalized:
                    variants.add(normalized.replace('Washington', 'Wn.'))
                    variants.add(normalized.replace('Washington', 'Wash.'))
        
        return list(variants)
    
    def calculate_similarity(self, name1: str, name2: str) -> float:
        """Calculate similarity between two case names."""
        if not name1 or not name2:
            return 0.0
        
        name1 = self._normalize_case_name(name1)
        name2 = self._normalize_case_name(name2)
        
        words1 = set(name1.lower().split())
        words2 = set(name2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def extract_case_name_from_context(self, text: str, citation: str, context_window: int = 500) -> Optional[str]:
        """Extract case name from text context around citation."""
        if not text or not citation:
            return None
        
        citation_pos = text.find(citation)
        if citation_pos == -1:
            return None
        
        start = max(0, citation_pos - context_window)
        end = min(len(text), citation_pos + len(citation) + context_window)
        context = text[start:end]
        
        for pattern in self.case_name_patterns:
            matches = re.findall(pattern, context, re.IGNORECASE)
            if matches:
                case_name = f"{matches[0][0]} v. {matches[0][1]}"
                if self._is_valid_case_name(case_name):
                    return case_name
        
        return None
    
    def _is_valid_case_name(self, case_name: str) -> bool:
        """Check if a case name is valid."""
        if not case_name or len(case_name) < 5:
            return False
        
        if not re.search(r'\bv\.?\b|\bvs\.?\b|\bversus\b', case_name, re.IGNORECASE):
            return False
        
        parts = re.split(r'\bv\.?\b|\bvs\.?\b|\bversus\b', case_name, flags=re.IGNORECASE)
        if len(parts) < 2:
            return False
        
        plaintiff = parts[0].strip()
        defendant = parts[1].strip()
        
        if not plaintiff or not defendant:
            return False
        
        return True
    
    def _normalize_case_name(self, case_name: str) -> str:
        """Normalize case name for comparison."""
        if not case_name:
            return ""
        
        case_name = re.sub(r'\s+', ' ', case_name.strip())
        
        case_name = re.sub(r'\bvs\.?\b', 'v.', case_name, flags=re.IGNORECASE)
        case_name = re.sub(r'\bversus\b', 'v.', case_name, flags=re.IGNORECASE)
        
        case_name = re.sub(r'^in\s+re\s+', '', case_name, flags=re.IGNORECASE)
        case_name = re.sub(r'\s+et\s+al\.?$', '', case_name, flags=re.IGNORECASE)
        
        return case_name
    
    def extract_enhanced_case_names(self, text: str) -> List[Dict]:
        """Extract enhanced case names with metadata from text."""
        case_names = []
        
        for pattern in self.case_name_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                plaintiff = match.group(1).strip()
                defendant = match.group(2).strip()
                case_name = f"{plaintiff} v. {defendant}"
                
                if self._is_valid_case_name(case_name):
                    case_info = {
                        'case_name': case_name,
                        'plaintiff': plaintiff,
                        'defendant': defendant,
                        'position': match.start(),
                        'confidence': self._calculate_case_name_confidence(case_name, text)
                    }
                    case_names.append(case_info)
        
        unique_cases = {}
        for case in case_names:
            normalized = self._normalize_case_name(case['case_name'])
            if normalized not in unique_cases or case['confidence'] > unique_cases[normalized]['confidence']:
                unique_cases[normalized] = case
        
        return sorted(unique_cases.values(), key=lambda x: x['confidence'], reverse=True)
    
    def _calculate_case_name_confidence(self, case_name: str, text: str) -> float:
        """Calculate confidence score for a case name."""
        confidence = 0.5  # Base confidence
        
        occurrences = text.lower().count(case_name.lower())
        if occurrences > 1:
            confidence += 0.2
        
        if self._has_citation_nearby(case_name, text):
            confidence += 0.3
        
        if len(case_name) > 20:
            confidence += 0.1
        
        return min(1.0, confidence)
    
    def _has_citation_nearby(self, case_name: str, text: str, window: int = 200) -> bool:
        """Check if there's a citation near the case name."""
        case_pos = text.lower().find(case_name.lower())
        if case_pos == -1:
            return False
        
        start = max(0, case_pos - window)
        end = min(len(text), case_pos + len(case_name) + window)
        context = text[start:end]
        
        for patterns in self.citation_patterns.values():
            for pattern in patterns:
                if re.search(pattern, context, re.IGNORECASE):
                    return True
        
        return False
    
    def extract_from_page_content(self, html_content: str, url: str, citation: str) -> Dict[str, Any]:
        """Extract information from page content."""
        if not html_content:
            return {'error': 'No HTML content provided'}
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            title = self._extract_title(soup)
            case_name = self._extract_case_name(soup, citation)
            court_info = self._extract_court_info(soup)
            date_info = self._extract_date_info(soup)
            
            content = self._extract_content(soup)
            
            return {
                'title': title,
                'case_name': case_name,
                'court': court_info,
                'date': date_info,
                'content': content,
                'url': url,
                'citation': citation,
                'extraction_method': 'page_content'
            }
            
        except Exception as e:
            logger.error(f"Error extracting from page content: {e}")
            return {'error': str(e)}
    
    def extract_from_legal_database(self, url: str, html_content: str) -> Dict[str, Any]:
        """Extract information from legal database pages."""
        domain = urlparse(url).netloc.lower()
        
        if 'casemine.com' in domain:
            return self._extract_casemine_info(html_content, url)
        elif 'vlex.com' in domain:
            return self._extract_vlex_info(html_content, url)
        elif 'casetext.com' in domain:
            return self._extract_casetext_info(html_content, url)
        elif 'leagle.com' in domain:
            return self._extract_leagle_info(html_content, url)
        elif 'justia.com' in domain:
            return self._extract_justia_info(html_content, url)
        elif 'findlaw.com' in domain:
            return self._extract_findlaw_info(html_content, url)
        else:
            return self._extract_generic_legal_info(html_content, url)
    
    def _extract_title(self, soup) -> str:
        """Extract title from HTML."""
        title_selectors = [
            'title',
            'h1',
            '.title',
            '.case-title',
            '.document-title',
            '[class*="title"]'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()
        
        return ""
    
    def _extract_case_name(self, soup, citation: str) -> str:
        """Extract case name from HTML."""
        case_name_selectors = [
            '.case-name',
            '.title',
            'h1',
            '[class*="case"]',
            '[class*="name"]'
        ]
        
        for selector in case_name_selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text().strip()
                if self._is_valid_case_name(text):
                    return text
        
        text_content = soup.get_text()
        return self.extract_case_name_from_context(text_content, citation) or ""
    
    def _extract_court_info(self, soup) -> str:
        """Extract court information from HTML."""
        court_selectors = [
            '.court',
            '.jurisdiction',
            '[class*="court"]',
            '[class*="jurisdiction"]'
        ]
        
        for selector in court_selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()
        
        return ""
    
    def _extract_date_info(self, soup) -> str:
        """Extract date information from HTML."""
        date_selectors = [
            '.date',
            '.decision-date',
            '[class*="date"]',
            'time'
        ]
        
        for selector in date_selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()
        
        return ""
    
    def _extract_content(self, soup) -> str:
        """Extract main content from HTML."""
        for script in soup(["script", "style"]):
            script.decompose()
        
        content_selectors = [
            '.content',
            '.main-content',
            '.document-content',
            'article',
            'main',
            '[class*="content"]'
        ]
        
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return soup.get_text().strip()
    
    def _extract_casemine_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Casemine pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court-name, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        date_elem = soup.select_one('.decision-date, .date')
        if date_elem:
            date = date_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'casemine'
        }
    
    def _extract_vlex_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Vlex pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.document-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'vlex'
        }
    
    def _extract_casetext_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Casetext pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'casetext'
        }
    
    def _extract_leagle_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Leagle pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'leagle'
        }
    
    def _extract_justia_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Justia pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'justia'
        }
    
    def _extract_findlaw_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from FindLaw pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'findlaw'
        }
    
    def _extract_generic_legal_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from generic legal pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = self._extract_title(soup)
        court = self._extract_court_info(soup)
        date = self._extract_date_info(soup)
        content = self._extract_content(soup)
        
        return {
            'title': title,
            'case_name': "",
            'court': court,
            'date': date,
            'content': content,
            'url': url,
            'source': 'generic'
        } 