"""
Web Extractor Module
Comprehensive web content extraction for legal documents and search results.
"""

import re
import logging
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class ComprehensiveWebExtractor:
    """Comprehensive web content extraction for legal documents and search results."""
    
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        self.citation_patterns = {
            'washington': [
                r'(\d+)\s+Wn\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Wn\.?\s*App\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Wash\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Washington\s+(\d+[a-z]?)\s+(\d+)',
            ],
            'federal': [
                r'(\d+)\s+U\.?S\.?\s+(\d+)',
                r'(\d+)\s+F\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+F\.?\s*Supp\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Fed\.?\s*(\d+[a-z]?)\s+(\d+)',
            ],
            'pacific': [
                r'(\d+)\s+P\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Pac\.?\s*(\d+[a-z]?)\s+(\d+)',
                r'(\d+)\s+Pacific\s+(\d+[a-z]?)\s+(\d+)',
            ]
        }
        
        self.case_name_patterns = [
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+v\.\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+vs\.\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+versus\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+et\s+al\.\s+v\.\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
        ]
    
    def generate_washington_variants(self, citation: str) -> List[str]:
        """Generate Washington citation variants."""
        variants = set()
        
        variants.add(citation)
        
        patterns = [
            (r'(\d+)\s+Wn\.?\s*(\d+[a-z]?)\s+(\d+)', r'\1 Wn. \2 \3'),
            (r'(\d+)\s+Wn\.?\s*App\.?\s*(\d+[a-z]?)\s+(\d+)', r'\1 Wn. App. \2 \3'),
            (r'(\d+)\s+Wash\.?\s*(\d+[a-z]?)\s+(\d+)', r'\1 Wash. \2 \3'),
            (r'(\d+)\s+Washington\s+(\d+[a-z]?)\s+(\d+)', r'\1 Washington \2 \3'),
        ]
        
        for pattern, replacement in patterns:
            if re.search(pattern, citation, re.IGNORECASE):
                normalized = re.sub(pattern, replacement, citation, flags=re.IGNORECASE)
                variants.add(normalized)
                
                if 'Wn.' in normalized:
                    variants.add(normalized.replace('Wn.', 'Wash.'))
                    variants.add(normalized.replace('Wn.', 'Washington'))
                elif 'Wash.' in normalized:
                    variants.add(normalized.replace('Wash.', 'Wn.'))
                    variants.add(normalized.replace('Wash.', 'Washington'))
                elif 'Washington' in normalized:
                    variants.add(normalized.replace('Washington', 'Wn.'))
                    variants.add(normalized.replace('Washington', 'Wash.'))
        
        return list(variants)
    
    def calculate_similarity(self, name1: str, name2: str) -> float:
        """Calculate similarity between two case names."""
        if not name1 or not name2:
            return 0.0
        
        name1 = self._normalize_case_name(name1)
        name2 = self._normalize_case_name(name2)
        
        words1 = set(name1.lower().split())
        words2 = set(name2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def extract_case_name_from_context(self, text: str, citation: str, context_window: int = 500) -> Optional[str]:
        """Extract case name from text context around citation."""
        if not text or not citation:
            return None
        
        citation_pos = text.find(citation)
        if citation_pos == -1:
            return None
        
        start = max(0, citation_pos - context_window)
        end = min(len(text), citation_pos + len(citation) + context_window)
        context = text[start:end]
        
        for pattern in self.case_name_patterns:
            matches = re.findall(pattern, context, re.IGNORECASE)
            if matches:
                case_name = f"{matches[0][0]} v. {matches[0][1]}"
                if self._is_valid_case_name(case_name):
                    return case_name
        
        return None
    
    def _is_valid_case_name(self, case_name: str) -> bool:
        """Check if a case name is valid."""
        if not case_name or len(case_name) < 5:
            return False
        
        if not re.search(r'\bv\.?\b|\bvs\.?\b|\bversus\b', case_name, re.IGNORECASE):
            return False
        
        parts = re.split(r'\bv\.?\b|\bvs\.?\b|\bversus\b', case_name, flags=re.IGNORECASE)
        if len(parts) < 2:
            return False
        
        plaintiff = parts[0].strip()
        defendant = parts[1].strip()
        
        if not plaintiff or not defendant:
            return False
        
        return True
    
    def _normalize_case_name(self, case_name: str) -> str:
        """Normalize case name for comparison."""
        if not case_name:
            return ""
        
        case_name = re.sub(r'\s+', ' ', case_name.strip())
        
        case_name = re.sub(r'\bvs\.?\b', 'v.', case_name, flags=re.IGNORECASE)
        case_name = re.sub(r'\bversus\b', 'v.', case_name, flags=re.IGNORECASE)
        
        case_name = re.sub(r'^in\s+re\s+', '', case_name, flags=re.IGNORECASE)
        case_name = re.sub(r'\s+et\s+al\.?$', '', case_name, flags=re.IGNORECASE)
        
        return case_name
    
    def extract_enhanced_case_names(self, text: str) -> List[Dict]:
        """Extract enhanced case names with metadata from text."""
        case_names = []
        
        for pattern in self.case_name_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                plaintiff = match.group(1).strip()
                defendant = match.group(2).strip()
                case_name = f"{plaintiff} v. {defendant}"
                
                if self._is_valid_case_name(case_name):
                    case_info = {
                        'case_name': case_name,
                        'plaintiff': plaintiff,
                        'defendant': defendant,
                        'position': match.start(),
                        'confidence': self._calculate_case_name_confidence(case_name, text)
                    }
                    case_names.append(case_info)
        
        unique_cases = {}
        for case in case_names:
            normalized = self._normalize_case_name(case['case_name'])
            if normalized not in unique_cases or case['confidence'] > unique_cases[normalized]['confidence']:
                unique_cases[normalized] = case
        
        return sorted(unique_cases.values(), key=lambda x: x['confidence'], reverse=True)
    
    def _calculate_case_name_confidence(self, case_name: str, text: str) -> float:
        """Calculate confidence score for a case name."""
        confidence = 0.5  # Base confidence
        
        occurrences = text.lower().count(case_name.lower())
        if occurrences > 1:
            confidence += 0.2
        
        if self._has_citation_nearby(case_name, text):
            confidence += 0.3
        
        if len(case_name) > 20:
            confidence += 0.1
        
        return min(1.0, confidence)
    
    def _has_citation_nearby(self, case_name: str, text: str, window: int = 200) -> bool:
        """Check if there's a citation near the case name."""
        case_pos = text.lower().find(case_name.lower())
        if case_pos == -1:
            return False
        
        start = max(0, case_pos - window)
        end = min(len(text), case_pos + len(case_name) + window)
        context = text[start:end]
        
        for patterns in self.citation_patterns.values():
            for pattern in patterns:
                if re.search(pattern, context, re.IGNORECASE):
                    return True
        
        return False
    
    def extract_from_page_content(self, html_content: str, url: str, citation: str) -> Dict[str, Any]:
        """Extract information from page content."""
        if not html_content:
            return {'error': 'No HTML content provided'}
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            title = self._extract_title(soup)
            case_name = self._extract_case_name(soup, citation)
            court_info = self._extract_court_info(soup)
            date_info = self._extract_date_info(soup)
            
            content = self._extract_content(soup)
            
            return {
                'title': title,
                'case_name': case_name,
                'court': court_info,
                'date': date_info,
                'content': content,
                'url': url,
                'citation': citation,
                'extraction_method': 'page_content'
            }
            
        except Exception as e:
            logger.error(f"Error extracting from page content: {e}")
            return {'error': str(e)}
    
    def extract_from_legal_database(self, url: str, html_content: str) -> Dict[str, Any]:
        """Extract information from legal database pages."""
        domain = urlparse(url).netloc.lower()
        
        if 'casemine.com' in domain:
            return self._extract_casemine_info(html_content, url)
        elif 'vlex.com' in domain:
            return self._extract_vlex_info(html_content, url)
        elif 'casetext.com' in domain:
            return self._extract_casetext_info(html_content, url)
        elif 'leagle.com' in domain:
            return self._extract_leagle_info(html_content, url)
        elif 'justia.com' in domain:
            return self._extract_justia_info(html_content, url)
        elif 'findlaw.com' in domain:
            return self._extract_findlaw_info(html_content, url)
        else:
            return self._extract_generic_legal_info(html_content, url)
    
    def _extract_title(self, soup) -> str:
        """Extract title from HTML."""
        title_selectors = [
            'title',
            'h1',
            '.title',
            '.case-title',
            '.document-title',
            '[class*="title"]'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()
        
        return ""
    
    def _extract_case_name(self, soup, citation: str) -> str:
        """Extract case name from HTML."""
        case_name_selectors = [
            '.case-name',
            '.title',
            'h1',
            '[class*="case"]',
            '[class*="name"]'
        ]
        
        for selector in case_name_selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text().strip()
                if self._is_valid_case_name(text):
                    return text
        
        text_content = soup.get_text()
        return self.extract_case_name_from_context(text_content, citation) or ""
    
    def _extract_court_info(self, soup) -> str:
        """Extract court information from HTML."""
        court_selectors = [
            '.court',
            '.jurisdiction',
            '[class*="court"]',
            '[class*="jurisdiction"]'
        ]
        
        for selector in court_selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()
        
        return ""
    
    def _extract_date_info(self, soup) -> str:
        """Extract date information from HTML."""
        date_selectors = [
            '.date',
            '.decision-date',
            '[class*="date"]',
            'time'
        ]
        
        for selector in date_selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()
        
        return ""
    
    def _extract_content(self, soup) -> str:
        """Extract main content from HTML."""
        for script in soup(["script", "style"]):
            script.decompose()
        
        content_selectors = [
            '.content',
            '.main-content',
            '.document-content',
            'article',
            'main',
            '[class*="content"]'
        ]
        
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return soup.get_text().strip()
    
    def _extract_casemine_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Casemine pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court-name, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        date_elem = soup.select_one('.decision-date, .date')
        if date_elem:
            date = date_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'casemine'
        }
    
    def _extract_vlex_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Vlex pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.document-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'vlex'
        }
    
    def _extract_casetext_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Casetext pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'casetext'
        }
    
    def _extract_leagle_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Leagle pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'leagle'
        }
    
    def _extract_justia_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from Justia pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'justia'
        }
    
    def _extract_findlaw_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from FindLaw pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = ""
        case_name = ""
        court = ""
        date = ""
        
        title_elem = soup.select_one('.case-title, h1')
        if title_elem:
            title = title_elem.get_text().strip()
        
        court_elem = soup.select_one('.court, .jurisdiction')
        if court_elem:
            court = court_elem.get_text().strip()
        
        return {
            'title': title,
            'case_name': case_name,
            'court': court,
            'date': date,
            'url': url,
            'source': 'findlaw'
        }
    
    def _extract_generic_legal_info(self, html_content: str, url: str) -> Dict[str, Any]:
        """Extract information from generic legal pages."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = self._extract_title(soup)
        court = self._extract_court_info(soup)
        date = self._extract_date_info(soup)
        content = self._extract_content(soup)
        
        return {
            'title': title,
            'case_name': "",
            'court': court,
            'date': date,
            'content': content,
            'url': url,
            'source': 'generic'
        } 